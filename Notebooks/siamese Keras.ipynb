{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('custom.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('./custom/s1.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./custom/s1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3yxf0dR4Hvo"
   },
   "source": [
    "!pip install imutils pandas tensorflow --index-url https://infyartifactory.ad.infosys.com/artifactory/api/pypi/pypi-remote/simple --trusted-host infyartifactory.ad.infosys.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AiMQd3PMytXB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import build_montages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os,random\n",
    "from PIL import Image,ImageOps\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "# for min pooling\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def cv2_imshow(mat):\n",
    "    plt.imshow(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MN1_4Jfx4Jyn",
    "outputId": "ea854a6c-81b7-45d5-9e08-02447bfae645"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 725, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread('./custom/full/001/001_00.png')\n",
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KVJFpXI6r4-Q"
   },
   "outputs": [],
   "source": [
    "def make_pairs():\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    pairImages = []\n",
    "    pairLabels = []\n",
    "    # calculate the total number of classes present in the dataset\n",
    "    # and then build a list of indexes for each class label that\n",
    "    # provides the indexes for all examples with a given label\n",
    "    # loop over all images\n",
    "    df = pd.read_csv('./custom/train_data.csv').sample(frac=0.5)\n",
    "    path = './custom/full'\n",
    "    for i in tqdm(range(len(df)//3)):\n",
    "        img1 = os.path.join(path, df.iloc[i, 0])\n",
    "        img1 = Image.open(img1)\n",
    "        img1 = img1.resize((725, 359))\n",
    "        img1 = ImageOps.grayscale(img1)\n",
    "        img1 = np.asarray(img1)/255.0\n",
    "        img2 = os.path.join(path, df.iloc[i, 1])\n",
    "        img2 = Image.open(img2)\n",
    "        img2 = img2.resize((725, 359))\n",
    "        img2 = ImageOps.grayscale(img2)\n",
    "        img2 = np.asarray(img2)/255.0\n",
    "        pairImages.append([img1, img2])\n",
    "        pairLabels.append([df.iloc[i, 2]])\n",
    "        \n",
    "    return (np.array(pairImages), np.array(pairLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PwNj8DFx1RSF"
   },
   "outputs": [],
   "source": [
    "def make_pairs_test():\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    pairImages = []\n",
    "    pairLabels = []\n",
    "    # calculate the total number of classes present in the dataset\n",
    "    # and then build a list of indexes for each class label that\n",
    "    # provides the indexes for all examples with a given label\n",
    "    # loop over all images\n",
    "    df = pd.read_csv('./custom/test_data.csv').sample(frac=0.5)\n",
    "    path = './custom/full'\n",
    "    for i in tqdm(range(len(df))):\n",
    "        img1 = os.path.join(path, df.iloc[i, 0])\n",
    "        img1 = Image.open(img1)\n",
    "        img1 = img1.resize((725, 359))\n",
    "        img1 = ImageOps.grayscale(img1)\n",
    "        img1 = np.asarray(img1)/255.0\n",
    "        img2 = os.path.join(path, df.iloc[i, 1])\n",
    "        img2 = Image.open(img2)\n",
    "        img2 = img2.resize((725, 359))\n",
    "        img2 = ImageOps.grayscale(img2)\n",
    "        img2 = np.asarray(img2)/255.0\n",
    "        pairImages.append([img1, img2])\n",
    "        pairLabels.append([df.iloc[i, 2]])\n",
    "        \n",
    "    return (np.array(pairImages), np.array(pairLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD3pclYW1Zns",
    "outputId": "d62a1445-9953-4802-d4d8-18eaf97cc461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing positive and negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47ac615ae89486aa0360cc8e09ae5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared\n"
     ]
    }
   ],
   "source": [
    "# build the positive and negative image pairs\n",
    "print(\"[INFO] preparing positive and negative pairs...\")\n",
    "(pairTest, labelTest) = make_pairs_test()\n",
    "#(pairTest, labelTest)[0]\n",
    "#images = []\n",
    "print('Prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fq3hpP8yzs0K",
    "outputId": "acdbeb30-0279-4790-ec7f-de6c9b4b70bb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing positive and negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba34b5090aec460689972b800a6a2b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared\n"
     ]
    }
   ],
   "source": [
    "# build the positive and negative image pairs\n",
    "print(\"[INFO] preparing positive and negative pairs...\")\n",
    "(pairTrain, labelTrain) = make_pairs()\n",
    "#(pairTrain, labelTrain)[0]\n",
    "#images = []\n",
    "print('Prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jmvulqh-z0g6"
   },
   "outputs": [],
   "source": [
    "# specify the shape of the inputs for our network\n",
    "IMG_SHAPE = (359,725,1)\n",
    "# specify the batch size and number of epochs\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-mxivswkMu35"
   },
   "outputs": [],
   "source": [
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"./output\"\n",
    "# use the base output path to derive the path to the serialized\n",
    "# model along with training history plot\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3_DAiHI1VChE"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    # unpack the vectors into separate lists\n",
    "    (featsA, featsB) = vectors\n",
    "    # compute the sum of squared distances between the vectors\n",
    "    sumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
    "        keepdims=True)\n",
    "    # return the euclidean distance between the vectors\n",
    "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "k2R02WK7BpBJ"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "    # explicitly cast the true class label data type to the predicted\n",
    "    # class label data type (otherwise we run the risk of having two\n",
    "    # separate data types, causing TensorFlow to error out)\n",
    "    y = tf.cast(y, preds.dtype)\n",
    "\n",
    "    # calculate the contrastive loss between the true labels and\n",
    "    # the predicted labels\n",
    "    squaredPreds = K.square(preds)\n",
    "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
    "\n",
    "    # return the computed contrastive loss to the calling function\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gtmpwwL5VOaW"
   },
   "outputs": [],
   "source": [
    "def plot_training(H, plotPath):\n",
    "    # construct a plot that plots and saves the training history\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Rb29VqjCRKBI"
   },
   "outputs": [],
   "source": [
    "# def build_siamese_model(inputShape, embeddingDim=48):\n",
    "#     # specify the inputs for the feature extractor network\n",
    "#     inputs = Input(inputShape)\n",
    "#     # define the first set of CONV => RELU => POOL => DROPOUT layers\n",
    "#     x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     # second set of CONV => RELU => POOL => DROPOUT layers\n",
    "#     x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "#     x = MaxPooling2D(pool_size=2)(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     # prepare the final outputs\n",
    "#     pooledOutput = GlobalAveragePooling2D()(x)\n",
    "#     outputs = Dense(embeddingDim)(pooledOutput)\n",
    "#     # build the model\n",
    "#     model = Model(inputs, outputs)\n",
    "#     # return the model to the calling function\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(inputShape, embeddingDim=60):\n",
    "\t# specify the inputs for the feature extractor network\n",
    "\tinputs = Input(inputShape)\n",
    "\t# define the first set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(32, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "\t# second set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(32, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # Third set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx =-K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # fourth set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # fifth set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # sixth set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # seventh set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(512, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "    # Eight set of CONV => RELU => POOL => DROPOUT layers\n",
    "\tx = Conv2D(512, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = -K.pool2d(-x, pool_size=(2, 2), strides=(2, 2))\n",
    "  # prepare the final outputs\n",
    "\tpooledOutput = GlobalAveragePooling2D()(x)\n",
    "\toutputs = Dense(embeddingDim)(pooledOutput)\n",
    "\t# build the model\n",
    "\tmodel = Model(inputs, outputs)\n",
    "\t# return the model to the calling function\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fjNjWhtSnxe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building siamese network...\n",
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Epoch 1/500\n",
      "109/109 [==============================] - 39s 341ms/step - loss: 0.6921 - accuracy: 0.5466 - val_loss: 0.6909 - val_accuracy: 0.5526\n",
      "Epoch 2/500\n",
      "109/109 [==============================] - 34s 316ms/step - loss: 0.6907 - accuracy: 0.5466 - val_loss: 0.6897 - val_accuracy: 0.5526\n",
      "Epoch 3/500\n",
      "109/109 [==============================] - 34s 309ms/step - loss: 0.6898 - accuracy: 0.5466 - val_loss: 0.6888 - val_accuracy: 0.5526\n",
      "Epoch 4/500\n",
      "109/109 [==============================] - 34s 311ms/step - loss: 0.6894 - accuracy: 0.5466 - val_loss: 0.6884 - val_accuracy: 0.5526\n",
      "Epoch 5/500\n",
      "109/109 [==============================] - 34s 313ms/step - loss: 0.6891 - accuracy: 0.5466 - val_loss: 0.6882 - val_accuracy: 0.5526\n",
      "Epoch 6/500\n",
      "109/109 [==============================] - 34s 311ms/step - loss: 0.6890 - accuracy: 0.5466 - val_loss: 0.6880 - val_accuracy: 0.5526\n",
      "Epoch 7/500\n",
      "109/109 [==============================] - 33s 305ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6879 - val_accuracy: 0.5526\n",
      "Epoch 8/500\n",
      "109/109 [==============================] - 34s 310ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6878 - val_accuracy: 0.5526\n",
      "Epoch 9/500\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.5466Epoch 10/500\n",
      "109/109 [==============================] - 34s 308ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 11/500\n",
      "109/109 [==============================] - 34s 310ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 12/500\n",
      "109/109 [==============================] - 34s 310ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 13/500\n",
      "109/109 [==============================] - 33s 308ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 14/500\n",
      "109/109 [==============================] - 34s 309ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 15/500\n",
      "109/109 [==============================] - 34s 309ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 16/500\n",
      "109/109 [==============================] - 33s 305ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 17/500\n",
      "109/109 [==============================] - 33s 305ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 18/500\n",
      "109/109 [==============================] - 33s 306ms/step - loss: 0.6889 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 19/500\n",
      "109/109 [==============================] - 34s 315ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 20/500\n",
      "109/109 [==============================] - 35s 318ms/step - loss: 0.6888 - accuracy: 0.5466 - val_loss: 0.6877 - val_accuracy: 0.5526\n",
      "Epoch 21/500\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.5466"
     ]
    }
   ],
   "source": [
    "# configure the siamese network\n",
    "print(\"[INFO] building siamese network...\")\n",
    "imgA = Input(shape=IMG_SHAPE)\n",
    "imgB = Input(shape=IMG_SHAPE)\n",
    "featureExtractor = build_siamese_model(IMG_SHAPE)\n",
    "featsA = featureExtractor(imgA)\n",
    "featsB = featureExtractor(imgB)\n",
    "# finally, construct the siamese network\n",
    "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
    "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
    "model = Model(inputs=[imgA, imgB], outputs=outputs)\n",
    "\n",
    "# compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\",#contrastive_loss\n",
    "    metrics=[\"accuracy\"])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(\n",
    "    [pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n",
    "    validation_data=([pairTest[:, 0], pairTest[:, 1]], labelTest[:]),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS)\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving siamese model...\")\n",
    "model.save(MODEL_PATH)\n",
    "# plot the training history\n",
    "print(\"[INFO] plotting training history...\")\n",
    "plot_training(history,PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = './custom/full'\n",
    "\n",
    "fold = random.choice(os.listdir(path)).replace('_forg','')\n",
    "\n",
    "org_fold = os.listdir(os.path.join(path,str(fold)))\n",
    "\n",
    "forg_fold = os.listdir(os.path.join(path,str(fold) + '_forg'))\n",
    "\n",
    "org1 = os.path.join(path,str(fold), random.choice(org_fold))\n",
    "org2 = os.path.join(path,str(fold), random.choice(org_fold))\n",
    "\n",
    "forg = os.path.join(path,str(fold) + '_forg', random.choice(forg_fold))\n",
    "\n",
    "\n",
    "imageA = cv2.imread(org1, 0)\n",
    "imageB = cv2.imread(org2, 0)\n",
    "\n",
    "origA = imageA.copy()\n",
    "origB = imageB.copy()\n",
    "\n",
    "# add a batch dimension to both images\n",
    "imageA = np.expand_dims(imageA, axis=0)\n",
    "imageB = np.expand_dims(imageB, axis=0)\n",
    "\n",
    "# scale the pixel values to the range of [0, 1]\n",
    "imageA = imageA / 255.0\n",
    "imageB = imageB / 255.0\n",
    "\n",
    "# use our siamese model to make predictions on the image pair,\n",
    "# indicating whether or not the images belong to the same class\n",
    "preds = model.predict([imageA, imageB])\n",
    "proba = preds[0][0]\n",
    "# initialize the figure\n",
    "fig = plt.figure(\"Pair #{}\".format(1), figsize=(4, 2))\n",
    "plt.suptitle(\"Similarity: {:.2f}\".format(proba))\n",
    "\n",
    "# show first image\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(origA, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# show the second image\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(origB, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print(org1,org1)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./custom/test_data.csv').sample(frac=0.5)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "siameseNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
